#!/usr/bin/env python3
"""
Diffusers inference (FLUX adapted from Transformers example)

Run any diffusers FLUX model for inference
"""

import sys
import os
from pathlib import Path
import shutil

import torch
from diffusers import FluxPipeline

UTILS_DIR = 'utils'

cr_log_file = 'cr.log'
wait_for_cr = False
compile = False
prompt = 'A cat holding a sign that says hello world'
trust_remote_code = False

exec(
    open(f'{UTILS_DIR}/configurator').read()
)   # overrides from command line or config file


def inference(model):
    try:
        generator_seed0 = torch.Generator("cpu").manual_seed(0)
        pipe = FluxPipeline.from_pretrained(
            model,
            torch_dtype=torch.bfloat16,
            trust_remote_code=trust_remote_code
        )
        pipe.to("cuda")

# FIXME: Should do warm up inference before final one
#         pipe(
#             prompt=prompt,
#             height=512,
#             width=512,
#             num_inference_steps=1,
#             guidance_scale=3.5,
#             generator=generator_seed0,
#         )

        if wait_for_cr:
            print('', flush=True)
            with open(cr_log_file, 'a') as cr_log:
                print('CHECKPOINT', file=cr_log, flush=True)
            with open(cr_log_file, 'r') as cr_log:
                while True:
                    line = cr_log.readline()
                    if 'RESTORE' in line:
                        break
                    if not line:
                        pass

        image_output1 = pipe(
            prompt=prompt,
            height=512,
            width=512,
            num_inference_steps=1,
            guidance_scale=3.5,
            generator=generator_seed0,
        ).images[0]
        image_output1.save("output.png")
        print(f"Generated output.png from prompt: \"{prompt}\"")

    except Exception as e:
        raise e
    finally:
        cleanup()
        if wait_for_cr:
            with open(cr_log_file, 'a') as cr_log: # Appending 'DONE'
                print('DONE', file=cr_log, flush=True)


def usage():
    print(f'Usage: {sys.argv[0]} <model>')
    sys.exit(1)

def cleanup():
    # Only clean if less than 100GB free
    # This will raise an error if shutil.disk_usage fails (e.g. / not accessible),
    # mirroring the directness of the original script.
    stat = shutil.disk_usage("/")
    free_gb = stat.free / (1024 ** 3)
    if free_gb > 100:
        return  # Enough space, skip cleanup

    cache_dirs = [
        Path.home() / ".cache" / "huggingface" / "transformers",
        Path.home() / ".cache" / "huggingface" / "hub",
    ]
    for cache_dir in cache_dirs:
        if cache_dir.exists():
            for item in cache_dir.iterdir():
                try:
                    if item.is_dir():
                        shutil.rmtree(item)
                    else: # Files or symlinks
                        item.unlink()
                except Exception as e:
                    print(f"Failed to remove {item}: {e}")


if __name__ == '__main__':
    if len(sys.argv) < 2:
        usage()
    model = None
    for arg in sys.argv[1:]:
        if not arg.startswith('--'):
            model = arg

    if model is None:
        usage()

    inference(model)
