apiVersion: v1
kind: Pod
metadata:
  generateName: simple-pytorch-
  namespace: default
spec:
  runtimeClassName: cedana
  restartPolicy: Never
  containers:
  - command:
    - bash
    - -c
    - |
      # Create the Python training script
      # num_workers set to 2, persistent_memory=True, 4 GPUs 
      mkdir -p /scripts
      mkdir -p /data/pytorch
      
      cat > /scripts/pytorch_cifar100_multigpu.py << 'EOF'
      import argparse
      import torch
      import torch.nn as nn
      import torch.nn.functional as F
      import torch.optim as optim
      from torchvision import datasets, transforms
      from torch.optim.lr_scheduler import StepLR
      import torch.distributed as dist
      import torch.multiprocessing as mp
      from torch.nn.parallel import DistributedDataParallel as DDP
      from torch.utils.data.distributed import DistributedSampler
      import os
      import time
      import numpy as np


      class Net(nn.Module):
          def __init__(self):
              super(Net, self).__init__()
              self.conv1 = nn.Conv2d(3, 32, 3, 1)  # Changed from 1 to 3 input channels for RGB
              self.conv2 = nn.Conv2d(32, 64, 3, 1)
              self.dropout1 = nn.Dropout(0.25)
              self.dropout2 = nn.Dropout(0.5)
              self.fc1 = nn.Linear(12544, 128)  # Corrected from 9216 to 12544 for CIFAR100
              self.fc2 = nn.Linear(128, 100)  # Changed from 10 to 100 classes for CIFAR100

          def forward(self, x):
              x = self.conv1(x)
              x = F.relu(x)
              x = self.conv2(x)
              x = F.relu(x)
              x = F.max_pool2d(x, 2)
              x = self.dropout1(x)
              x = torch.flatten(x, 1)
              x = self.fc1(x)
              x = F.relu(x)
              x = self.dropout2(x)
              x = self.fc2(x)
              output = F.log_softmax(x, dim=1)
              return output


      def setup_distributed(rank, world_size):
          os.environ['MASTER_ADDR'] = 'localhost'
          os.environ['MASTER_PORT'] = '12355'
          dist.init_process_group("nccl", rank=rank, world_size=world_size)


      def cleanup_distributed():
          dist.destroy_process_group()


      def train(args, model, device, train_loader, optimizer, epoch, rank=0, world_size=1):
          model.train()
          epoch_start_time = time.time()
          epoch_loss = 0
          batch_count = 0
          
          for batch_idx, (data, target) in enumerate(train_loader):
              batch_start_time = time.time()
              data, target = data.to(device), target.to(device)
              optimizer.zero_grad()
              output = model(data)
              loss = F.nll_loss(output, target)
              loss.backward()
              optimizer.step()
              
              epoch_loss += loss.item()
              batch_count += 1
              
              if batch_idx % args.log_interval == 0 and rank == 0:
                  batch_time = time.time() - batch_start_time
                  print('Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}\tBatch Time: {:.4f}s'.format(
                      epoch, batch_idx * len(data) * world_size, len(train_loader.dataset),
                      100. * batch_idx / len(train_loader), loss.item(), batch_time))
                  if args.dry_run:
                      break
          
          epoch_time = time.time() - epoch_start_time
          avg_loss = epoch_loss / batch_count if batch_count > 0 else 0
          
          return epoch_time, avg_loss


      def test(model, device, test_loader, rank=0):
          model.eval()
          test_loss = 0
          correct = 0
          start_time = time.time()
          
          with torch.no_grad():
              for data, target in test_loader:
                  data, target = data.to(device), target.to(device)
                  output = model(data)
                  test_loss += F.nll_loss(output, target, reduction='sum').item()
                  pred = output.argmax(dim=1, keepdim=True)
                  correct += pred.eq(target.view_as(pred)).sum().item()

          test_loss /= len(test_loader.dataset)
          test_time = time.time() - start_time

          if rank == 0:
              print('\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%), Test Time: {:.4f}s\n'.format(
                  test_loss, correct, len(test_loader.dataset),
                  100. * correct / len(test_loader.dataset), test_time))
          
          return test_time


      def main_worker(gpu, args):
          setup_distributed(gpu, args.world_size)
          torch.manual_seed(args.seed)
          torch.cuda.set_device(gpu)
          device = torch.device(f"cuda:{gpu}")
          
          # CIFAR100 normalization values
          transform = transforms.Compose([
              transforms.ToTensor(),
              transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))
          ])
          
          # Use CIFAR100 dataset instead of MNIST
          dataset1 = datasets.CIFAR100('/data/pytorch', train=True, download=True, transform=transform)
          dataset2 = datasets.CIFAR100('/data/pytorch', train=False, transform=transform)
          
          train_sampler = DistributedSampler(dataset1, num_replicas=args.world_size, rank=gpu)
          test_sampler = DistributedSampler(dataset2, num_replicas=args.world_size, rank=gpu)
          
          train_kwargs = {'batch_size': args.batch_size, 'num_workers': 2, 'pin_memory': True, 'sampler': train_sampler, 'shuffle': False}
          test_kwargs = {'batch_size': args.test_batch_size, 'num_workers': 2, 'pin_memory': True, 'sampler': test_sampler, 'shuffle': False}
          # train_kwargs = {'batch_size': args.batch_size, 'num_workers': 0}
          # test_kwargs = {'batch_size': args.test_batch_size, 'num_workers': 0}
          
          train_loader = torch.utils.data.DataLoader(dataset1, **train_kwargs)
          test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)
          
          model = Net().to(device)
          model = DDP(model, device_ids=[gpu])
          
          optimizer = optim.Adadelta(model.parameters(), lr=args.lr)
          scheduler = StepLR(optimizer, step_size=1, gamma=args.gamma)
          
          epoch_times = []
          total_start_time = time.time()
          
          for epoch in range(1, args.epochs + 1):
              train_sampler.set_epoch(epoch)
              epoch_time, avg_loss = train(args, model, device, train_loader, optimizer, epoch, gpu, args.world_size)
              test_time = test(model, device, test_loader, gpu)
              scheduler.step()
              
              if gpu == 0:
                  epoch_times.append(epoch_time)
                  print(f'Epoch {epoch} - Train Time: {epoch_time:.4f}s, Test Time: {test_time:.4f}s, Avg Loss: {avg_loss:.6f}')
          
          total_time = time.time() - total_start_time
          if gpu == 0:
              avg_epoch_time = np.mean(epoch_times) if epoch_times else 0
              print(f'\nTotal Training Time: {total_time:.4f}s')
              print(f'Average Time per Epoch: {avg_epoch_time:.4f}s')
              print(f'Epoch Times: {[f"{t:.4f}s" for t in epoch_times]}')
          
          if args.save_model and gpu == 0:
              torch.save(model.module.state_dict(), "cifar100_cnn_multigpu.pt")
          
          cleanup_distributed()


      def train_single_gpu(args):
          use_cuda = not args.no_cuda and torch.cuda.is_available()
          torch.manual_seed(args.seed)
          device = torch.device("cuda" if use_cuda else "cpu")
          
          train_kwargs = {'batch_size': args.batch_size}
          test_kwargs = {'batch_size': args.test_batch_size}
          if use_cuda:
              # accel_kwargs = {'num_workers': 1, 'persistent_workers': True, 'pin_memory': True, 'shuffle': True}
              accel_kwargs = {'num_workers': 0}
              train_kwargs.update(accel_kwargs)
              test_kwargs.update(accel_kwargs)
          
          # CIFAR100 normalization values
          transform = transforms.Compose([
              transforms.ToTensor(),
              transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))
          ])
          
          # Use CIFAR100 dataset instead of MNIST
          dataset1 = datasets.CIFAR100('/data/pytorch', train=True, download=True, transform=transform)
          dataset2 = datasets.CIFAR100('/data/pytorch', train=False, transform=transform)
          train_loader = torch.utils.data.DataLoader(dataset1, **train_kwargs)
          test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)
          
          model = Net().to(device)
          optimizer = optim.Adadelta(model.parameters(), lr=args.lr)
          
          scheduler = StepLR(optimizer, step_size=1, gamma=args.gamma)
          
          epoch_times = []
          total_start_time = time.time()
          
          for epoch in range(1, args.epochs + 1):
              epoch_time, avg_loss = train(args, model, device, train_loader, optimizer, epoch)
              test_time = test(model, device, test_loader)
              scheduler.step()
              
              epoch_times.append(epoch_time)
              print(f'Epoch {epoch} - Train Time: {epoch_time:.4f}s, Test Time: {test_time:.4f}s, Avg Loss: {avg_loss:.6f}')
          
          total_time = time.time() - total_start_time
          avg_epoch_time = np.mean(epoch_times) if epoch_times else 0
          print(f'\nTotal Training Time: {total_time:.4f}s')
          print(f'Average Time per Epoch: {avg_epoch_time:.4f}s')
          print(f'Epoch Times: {[f"{t:.4f}s" for t in epoch_times]}')
          
          if args.save_model:
              torch.save(model.state_dict(), "cifar100_cnn.pt")


      def main():
          parser = argparse.ArgumentParser(description='PyTorch CIFAR100 Example - Single/Multi-GPU Version')
          parser.add_argument('--batch-size', type=int, default=64, metavar='N',
                              help='input batch size for training (default: 64)')
          parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',
                              help='input batch size for testing (default: 1000)')
          parser.add_argument('--epochs', type=int, default=14, metavar='N',
                              help='number of epochs to train (default: 14)')
          parser.add_argument('--lr', type=float, default=1.0, metavar='LR',
                              help='learning rate (default: 1.0)')
          parser.add_argument('--gamma', type=float, default=0.7, metavar='M',
                              help='Learning rate step gamma (default: 0.7)')
          parser.add_argument('--no-cuda', action='store_true', default=False,
                              help='disables CUDA training')
          parser.add_argument('--dry-run', action='store_true', default=False,
                              help='quickly check a single pass')
          parser.add_argument('--seed', type=int, default=1, metavar='S',
                              help='random seed (default: 1)')
          parser.add_argument('--log-interval', type=int, default=10, metavar='N',
                              help='how many batches to wait before logging training status')
          parser.add_argument('--save-model', action='store_true', default=False,
                              help='For Saving the current Model')
          parser.add_argument('--num-gpus', type=int, default=-1, metavar='N',
                              help='number of GPUs to use (default: -1, use all available)')
          args = parser.parse_args()
          
          use_cuda = not args.no_cuda and torch.cuda.is_available()
          
          if not use_cuda:
              print("CUDA is not available. Running on CPU.")
              train_single_gpu(args)
              return
          
          if args.num_gpus == -1:
              args.world_size = torch.cuda.device_count()
          else:
              args.world_size = min(args.num_gpus, torch.cuda.device_count())
          
          print(f"Using {args.world_size} GPUs")
          
          if args.world_size <= 1:
              print("Using single GPU mode")
              train_single_gpu(args)
              return
          
          original_batch_size = args.batch_size
          original_test_batch_size = args.test_batch_size
          args.batch_size = args.batch_size // args.world_size
          args.test_batch_size = args.test_batch_size // args.world_size
          
          print(f"Adjusted batch size: {args.batch_size} (original: {original_batch_size})")
          print(f"Adjusted test batch size: {args.test_batch_size} (original: {original_test_batch_size})")
          
          mp.spawn(main_worker, nprocs=args.world_size, args=(args,))


      if __name__ == '__main__':
          main()
      EOF

      # Execute the training script
      python3 /scripts/pytorch_cifar100_multigpu.py --epochs 50 --num-gpus 4
    env:
    - name: DEPLOY_VALUE
      value: "4"
    - name: PYTHONUNBUFFERED
      value: "1"  
    - name: PYTHONIOENCODING
      value: "utf-8" 
    image: nvcr.io/nvidia/pytorch:25.03-py3
    imagePullPolicy: IfNotPresent
    name: simple-pytorch
    resources:
      limits:
        nvidia.com/gpu: 4
      requests:
        nvidia.com/gpu: 4
    volumeMounts:
    - mountPath: /dev/shm
      name: devshm
    - mountPath: /data
      name: data
    securityContext:
      capabilities:
        add: ["IPC_LOCK"]
  volumes:
  - name: devshm
    emptyDir:
      sizeLimit: 200Gi
   
  - name: data
    persistentVolumeClaim:
      claimName: dgtest-pvc
