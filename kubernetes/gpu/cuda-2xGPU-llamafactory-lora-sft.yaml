apiVersion: v1
kind: Pod
metadata:
  generateName: dgtest-llamafac-
  namespace: default
spec:
  runtimeClassName: cedana
  restartPolicy: Never
  containers:
    - command:
        - bash
        - -c
        - |
          git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git
          cd LLaMA-Factory
          export PIP_CONSTRAINT=""
          pip install -e ".[torch,metrics]"
          # sleep 1d;
          export FORCE_TORCHRUN=1
          llamafactory-cli train examples/train_lora/llama3_lora_sft.yaml learning_rate=1e-5 logging_steps=1
      env:
        - name: DEPLOY_VALUE
          value: "4"
        - name: HF_HOME
          value: "/data/huggingface"
        - name: HF_HUB_DISABLE_XET
          value: "1"
        # - name: HF_HUB_OFFLINE
        #   value: "1"
        - name: HF_TOKEN
          valueFrom:
            secretKeyRef:
              name: hf-token-secret
              key: HF_TOKEN
      image: nvcr.io/nvidia/pytorch:25.03-py3
      imagePullPolicy: Always
      name: dgtest-megatronlm
      resources:
        limits:
          nvidia.com/gpu: 2
        requests:
          nvidia.com/gpu: 2
      volumeMounts:
        - mountPath: /data
          name: data
      securityContext:
        capabilities:
          add: ["IPC_LOCK"]

  volumes:
    - name: data
      persistentVolumeClaim:
        claimName: dgtest-pvc
