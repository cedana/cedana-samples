apiVersion: v1
kind: Pod
metadata:
  generateName: vllm
  namespace: default
spec:
  runtimeClassName: cedana
  restartPolicy: Never
  containers:
  - command:
    - bash
    - -c
    - |
      vllm serve meta-llama/Llama-3.1-8B --port 8000;
    env:
    - name: DEPLOY_VALUE
      value: "2"
    - name: HF_HOME
      value: /data/huggingface
    - name: HF_HUB_DISABLE_XET
      value: "1"
    # - name: HF_HUB_OFFLINE
    #   value: "1"
    - name: HF_TOKEN
      valueFrom:
        secretKeyRef:
          name: hf-token-secret
          key: HF_TOKEN
    image: nvcr.io/nvidia/tritonserver:25.06-vllm-python-py3
    imagePullPolicy: IfNotPresent
    name: dgtest-vllm
    resources:
      limits:
        nvidia.com/gpu: 4
      requests:
        nvidia.com/gpu: 4
    volumeMounts:
    #   mountPath: /scripts
    - mountPath: /dev/shm
      name: devshm
    - mountPath: /data
      name: data
    securityContext:
      capabilities:
        add: ["IPC_LOCK"]
    ports:
      - containerPort: 8000
    readinessProbe:
      tcpSocket:
        port: 8000
      initialDelaySeconds: 1
      periodSeconds: 1
      timeoutSeconds: 1
  volumes:
  - name: devshm
    emptyDir:
      sizeLimit: 200Gi
      medium: Memory
  - name: data
    persistentVolumeClaim:
      claimName: dgtest-pvc
