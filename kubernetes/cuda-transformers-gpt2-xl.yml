apiVersion: v1
kind: Pod
metadata:
  name: transformers-gpt2-xl
  namespace: default
spec:
  runtimeClassName: cedana # required for GPU C/R support (use nvidia for native)
  containers:
    - name: transformers-gpt2-xl
      image: cedana/cedana-samples:cuda12.4-torch2.5
      command:
        - python3
        - -u
        - /app/gpu_smr/pytorch/llm/transformers_inference.py
        - --readiness-port=8888
        - --model=gpt2-xl
      ports:
        - containerPort: 8888
      readinessProbe:
        tcpSocket:
          port: 8888
        initialDelaySeconds: 0
        periodSeconds: 1
      resources:
        limits:
          nvidia.com/gpu: 1
