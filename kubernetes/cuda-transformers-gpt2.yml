apiVersion: apps/v1
kind: Pod
metadata:
  name: transformers-gpt2
  namespace: default
spec:
  runtimeClassName: cedana # required for GPU C/R support (use nvidia for native)
  containers:
    - name: transformers-gpt2
      image: cedana/cedana-samples:cuda12.4-torch2.5
      command:
        [
          "python3",
          "-u",
          "/app/gpu_smr/pytorch/llm/transformers_inference.py",
          "--model",
          "gpt2",
        ]
      resources:
        limits:
          nvidia.com/gpu: 1
