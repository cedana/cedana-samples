apiVersion: v1
kind: Pod
metadata:
  generateName: vllm-llama-3-1-8b-
  namespace: default
spec:
  runtimeClassName: cedana # required for GPU C/R support (use nvidia for native)
  containers:
    - name: vllm-llama-8b
      image: cedana/cedana-samples-test:cuda12.4-torch2.7
      command:
        - python3
        - -u
        - gpu_smr/pytorch/llm/vllm_inference_readiness.py
        - --readiness-port=8888
        - --model=meta-llama/Llama-3.1-8B
      env:
        - name: LD_LIBRARY_PATH
          value:
            /usr/local/lib
        - name: HF_TOKEN
          value:
      ports:
        - containerPort: 8888
      readinessProbe:
        tcpSocket:
          port: 8888
        initialDelaySeconds: 1
        periodSeconds: 1
        timeoutSeconds: 1
      resources:
        limits:
          nvidia.com/gpu: 1
