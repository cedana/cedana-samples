apiVersion: v1
kind: Pod
metadata:
  generateName: vllm-llama-3.1-8B-
  namespace: default
spec:
  runtimeClassName: cedana # required for GPU C/R support (use nvidia for native)
  containers:
    - name: vllm-llama-8b
      image: cedana/cedana-samples:cuda12.4-torch2.5
      command:
        - python3
        - -u
        - /app/gpu_smr/pytorch/llm/vllm_inference_readiness.py
        - --readiness-port=8888
        - --model=meta-llama/Llama-3.1-8B
      ports:
        - containerPort: 8888
      readinessProbe:
        tcpSocket:
          port: 8888
        initialDelaySeconds: 1
        periodSeconds: 1
        timeoutSeconds: 1
      resources:
        limits:
          nvidia.com/gpu: 1
